# 动手学深度学习

## 1. 深度学习简介

+ 通俗来说，机器学习是一门讨论各式各样的适用于不同问题的函数形式，以及如何使用数据来有效地获取函数参数具体值的学科。
+ 深度学习是指机器学习中的一类函数，它们的形式通常为多层神经网络。
+ 赫布理论（神经是通过正向强化来学习的）是感知机学习算法的原型，并成为支撑今日深度学习的**随机梯度下降算法**的基石：强化合意的行为、惩罚不合意的行为，最终获得优良的神经网络参数。
+ 绝大多数神经网络都包含以下的核心原则。
  - 交替使用线性处理单元与非线性处理单元，它们经常被称为“层”。
  - 使用链式法则（即反向传播）来更新网络的参数。
+ 在数据和计算力的稀缺时，从经验上说，如核方法、决策树和**概率图模型**等统计工具更优。它们不像神经网络一样需要长时间的训练，并且在强大的理论保证下提供可以预测的结果。
+ 注意力机制、生成对抗网络
+ 机器学习和深度学习的关系
  + 机器学习研究如何使计算机系统利用经验改善性能，。它是人工智能领域的分支，也是实现人工智能的一种手段。
  + 在机器学习的众多研究方向中，表征学习关注如何自动找出表示数据的合适方式，以便更好地将输入变换为正确的输出。
  + 深度学习是具有多级表示的表征学习方法。在每一级（从原始数据开始），深度学习通过简单的函数将该级的表示变换为更高级的表示。因此，深度学习模型也可以看作是由许多简单函数复合而成的函数。当这些复合的函数足够多时，深度学习模型就可以表达非常复杂的变换。
+ 深度学习可以逐级表示越来越抽象的概念或模式，作为表征学习的一种，深度学习将自动找出每一级表示数据的合适方式。
+ 深度学习的一个外在特点是**端到端的训练**。并不是将单独调试的部分拼凑起来组成一个系统，而是将整个系统组建好之后一起训练。
+ 从含参数统计模型转向完全无参数的模型。当数据非常稀缺时，需要通过简化对现实的假设来得到实用的模型；当数据充足时，可以用能更好地拟合现实的无参数模型来替代这些含参数模型。这可以得到更精确的模型，尽管需要牺牲一些可解释性。
+ 相对其它经典的机器学习方法而言，深度学习的不同在于：对非最优解的包容、对非凸非线性优化的使用，以及勇于尝试没有被证明过的方法。这种在处理统计问题上的新经验主义吸引了大量人才的涌入，使得大量实际问题有了更好的解决方案。

## 2. 预备知识

### 2.1 环境配置

#### 2.1.1 Anaconda

+ [下载并安装Anaconda](https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/)

+ 常用的几个conda命令

  ```bash
    # 创建虚拟环境
    conda create -n pytorch python=3.6
    # 删除虚拟环境
    conda env remove -n pytorch
    # 激活指定虚拟环境
    conda activate pytorch
    # 退出当前虚拟环境
    conda deactivate
    # 安装包，如numpy（提前进入目标环境）
    conda install numpy
    或
    pip install numpy
    # 安装指定版本的包，如numpy
    pip install numpy==1.6.0
    # 升级包
    pip install --upgrade numpy
    # 卸载包
    pip uninstall numpy
    # 查看conda中所有环境
    conda info -e
    # 查看某个环境中安装的所有包（提前进入目标环境）
    conda list
  ```

+ 修改镜像为国内源，如[清华源](https://mirror.tuna.tsinghua.edu.cn/help/anaconda/)

#### 2.1.2 Jupyter

+ 安装Anaconda软件之后，在首页找到Jupyter点击Install即可。

+ 不同虚拟环境下各自的Jupyter，只需提前在Anaconda软件界面切换虚拟环境即可。![image-20211208170503225](C:\Users\Q\AppData\Roaming\Typora\typora-user-images\image-20211208170503225.png)

+ 为不同的jupyter指定不同的工作路径，首先为这个jupyter新增一个本地工作路径`path`，找到“开始”处新增的Jupyter Notebook (pytorch)，右键打开文件路径，右键属性，修改目标，把`"%USERPROFILE"`修改为指定路径path即可，如图。

  <img src="C:\Users\Q\AppData\Roaming\Typora\typora-user-images\image-20211208170823226.png" alt="image-20211208170823226" style="zoom: 33%;" /><img src="C:\Users\Q\AppData\Roaming\Typora\typora-user-images\image-20211208171047220.png" alt="image-20211208171047220" style="zoom:33%;" />
  
+ 修改后单击该快捷方式即可进入网页使用JupyterNoteBook

#### 2.1.3 PyTorch

+ win10系统，进入cmd

  ```bash
  # 创建名为pytorch的虚拟环境
  conda create --name pytorch python=3.6
  # 激活名为pytorch的虚拟环境
  conda activate pytorch
  # 在pytorch虚拟环境中使用官网提示的命令安装PyTorch
  conda install pytorch torchvision torchaudio cpuonly -c pytorch
  ```

+ 命令行输入`python`进入python，并输入下面代码可查看pytorch是否安装成功：

    ```python
    import torch
    import torchvision
    print(torch.__version)
    ```

  ![image-20211208164303307](C:\Users\Q\AppData\Roaming\Typora\typora-user-images\image-20211208164303307.png)

### 2.2 数据操作

+ 在PyTorch中，`torch.Tensor`是存储和变换数据的主要工具。`Tensor`提供GPU计算和自动求梯度等更多功能，使`Tensor`更加适合深度学习。
+ [Tensor官方文档](https://pytorch.org/docs/stable/tensors.html)

### 2.3 自动求梯度

- 如果将Tensor的属性 .requires_grad 设置为True，它将开始追踪(track)在Tensor上的所有操作（这样就可以利用链式法则进行梯度传播了）。
- 完成计算后，可以调用.backward()来完成所有梯度计算。
- 此Tensor的梯度将累积到 .grad 属性中。
- 如果不想被继续追踪，可以调用 .detach() 将其从追踪记录中分离出来，以防止此后的计算被追踪，这样梯度就传不过去了。
- 还可以用 with torch.no_grad() 将不想被追踪的操作代码块包裹起来，此方法在评估模型的时候常用，因为在评估模型时不需要计算可训练参数（requires_grad=True）的梯度。
- `Tensor`和`Function`互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG）。
- 每个Tensor都有一个.grad_fn 属性，该属性就是创建该Tensor的Function，可判断该Tensor是不是通过某些运算得到的：若是，则grad_fn返回一个与这些运算相关的对象，若否，则返回None。

+ 数学上，如果有一个函数值和自变量都为向量的函数y-> = f(x->)，那么y->关于x->的梯度就是一个雅可比矩阵（Jacobian matrix） 包 torch.autograd 可用于计算一些雅克比矩阵的乘积

+ grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零

+ 不允许张量对张量求导，只允许标量对张量求导，求导结果是和自变量同形的张量

+ 所以必要时要把张量通过将所有张量的元素加权求和的方式转换为标量

  例：设y由自变量x计算而来，w是和y同形的张量，则y.backward(w)的含义是：先计算t = torch.sum(y * w)，则t是个标量，然后求t对自变量x的导数


## 3. 深度学习基础

### 3.1 线性回归

+ 线性回归输出的是一个连续值，因此适用于回归问题（如预测房屋价格、气温、销售额等连续值的问题）

  <img src="C:\Users\Q\AppData\Roaming\Typora\typora-user-images\image-20211209092913654.png" alt="image-20211209092913654" style="zoom:50%;" />

+ 分类问题中模型输出的是一个离散值（如图像分类、疾病检测）。softmax回归则适用于分类问题。 

+ 训练数据集（training data set）或训练集（training set），样本（sample），标签（label），用来预测标签的因素叫作特征（feature）。特征用来表征样本的特点

+ ==损失函数==：

  + 在模型训练中，需要衡量预测值与真实值之间的误差。通常会选取一个非负数作为误差，数值越小表示误差越小。一个常用的选择是平方函数。误差越小表示预测值与真实值越相近，二者相等时误差为0。给定训练数据集，则误差只与模型参数相关，因此将它记为以模型的参数为参数的函数。样本误差：<img src="C:\Users\Q\AppData\Roaming\Typora\typora-user-images\image-20211209091911287.png" alt="image-20211209091911287" style="zoom:50%;" />
  + 在机器学习中，将衡量误差的函数称为**损失函数（loss function）**。平方误差函数也称为**平方损失（square loss）**。
  + 通常用训练数据集中**所有样本误差的平均值**来衡量**模型预测的质量**![image-20211209092012558](C:\Users\Q\AppData\Roaming\Typora\typora-user-images\image-20211209092012558.png)
  + 在模型训练中，总是希望找出一组模型参数，记为 w^∗^1, w^∗^2, b^∗^,来使训练样本平均损失最小<img src="C:\Users\Q\AppData\Roaming\Typora\typora-user-images\image-20211209092130149.png" alt="image-20211209092130149" style="zoom:50%;" />

+ 当模型和损失函数形式较为简单时，误差最小化问题的解可以直接用公式表达。这类解叫作**解析解（analytical solution）**。线性回归和平方误差都属于解析解。但大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作**数值解（numerical solution）**。

+ 在求数值解的优化算法中，`小批量随机梯度下降（mini-batch stochastic gradient descent）`在深度学习中被广泛使用：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）B，然后求小批量中数据样本的平均损失关于模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。、

+ 在训练本节讨论的线性回归模型的过程中，模型的每个参数将作如下迭代![image-20211209092950922](C:\Users\Q\AppData\Roaming\Typora\typora-user-images\image-20211209092950922.png)

+ 在上式中，| B |代表`每个小批量中的样本个数（batch size）`，η 称作`学习率（learning rate）`并取正数。此处的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作`超参数（hyperparameter）`。通常所说的==**“调参”指的正是调节超参数**==，例如通过==反复试错==来找到超参数合适的值。

+ 模型训练完成后，我们将模型参数 w~1~,w~2~,b 在优化算法停止时的值分别记作 wˆ~1~,wˆ~2~,bˆ。此处得到的不一定是最小化损失函数的最优解 w∗~1~,w∗~2~,b∗而是**最优解的一个近似**。然后，就可以使用训练得到的线性回归模型来估算（即`模型预测`、模型推断或模型测试）训练数据集以外任意一个样例的值了。

+ 在深度学习中，可以使用**神经网络图**直观地表现模型结构。神经网络图隐去了模型参数权重和偏差。

  <img src="C:\Users\Q\AppData\Roaming\Typora\typora-user-images\image-20211209094142338.png" alt="image-20211209094142338" style="zoom:50%;" />

  

+ 在图3.1所示的神经网络中，输入分别为 x~1~ 和 x~2~，因此输入层的输入个数为2。**输入个数**也叫`特征数`或`特征向量维度`。图3.1中网络的输出为 o，输出层的输出个数为1。直接将图3.1中神经网络的输出 o 作为线性回归的输出，即 yˆ=o。由于输入层并不涉及计算，按照惯例，图3.1所示的神经网络的层数为1。所以线性回归是一个单层神经网络。输出层中负责计算 o 的单元又叫`神经元`。在线性回归中，输出层中的神经元和输入层中各个输入完全连接。因此输出层又叫`全连接层（fully-connected layer）`或`稠密层（dense layer）`。

+ 广义上讲，当数据样本数为 n，特征数为 d 时，线性回归的矢量计算表达式为：**yˆ** = **Xω** + b，其中模型输出yˆ∈R^n×1^，批量数据样本特征X∈R^n×d^，权重w∈R^d×1^， 偏差 b∈R。相应地，批量数据样本标签 y∈R^n×1^。

  设模型参数 θ=[w~1~,w~2~,b]^T^，重写损失函数为 ℓ(**θ**) = (1/ 2n)(**y**^ − **y**)^⊤^(**y**^−**y**)

  小批量随机梯度下降的**迭代步骤**相应地改写为**θ**←**θ**−(η/|B|)∑~i∈B~∇~θ~ℓ^(i)^(**θ**)

+ 和大多数深度学习模型一样，对于线性回归这样一种单层神经网络，它的基本要素包括`模型`、`训练数据`、`损失函数`和`优化算法`。

+ 既可以用神经网络图表示线性回归，也可以用矢量计算表示该模型。

+ 应该尽可能采用矢量计算，以提升计算效率。

 ### 3.2 线性回归的从零实现

+  噪声代表了数据集中无意义的干扰
+ 迭代周期个数`num_epochs`和学习率`lr`都是超参数
+ 在实践中，大多超参数都需要通过反复试错来不断调节
+ 迭代周期数设得越大模型可能越有效，但是训练时间可能过长

### 3.3 线性回归的简洁实现

+ `torch.utils.data`模块提供了有关数据处理的工具，

  `torch.nn`模块定义了大量神经网络的层，

  `torch.nn.init`模块定义了各种初始化方法，

  `torch.optim`模块提供了很多常用的优化算法。

+ `torch.nn`模块，“nn”是neural networks（神经网络）的缩写。`nn`利用`autograd`来定义模型。`nn`的核心数据结构是`Module`，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。实际使用中最常见的做法是继承`nn.Module`，撰写自己的层/网络。一个`nn.Module`实例应该包含一些层以及返回输出的前向传播（forward）方法。

+ 可以用`nn.Sequential`更加方便地搭建网络，`Sequential`是一个有序的容器，网络层将按照在传入`Sequential`的顺序依次被添加到计算图中

+ PyTorch在`nn`模块中提供了各种损失函数，这些损失函数可看作是一种特殊的层，PyTorch也将这些损失函数实现为`nn.Module`的子类。如：

  ```python
  loss = nn.MSELoss()
  ```

+ `torch.optim`模块提供了很多常用的优化算法比如SGD、Adam和RMSProp等

  ```python
  # 创建一个用于优化net所有参数的优化器实例
  # 指定学习率为0.03的小批量随机梯度下降（SGD）为优化算法。
  import torch.optim as optim
  optimizer = optim.SGD(net.parameters(), lr=0.03)print(optimizer)
  ```

### 3.4 softmax回归

+ 对于离散值预测问题，可以使用诸如softmax回归在内的分类模型。

+ 和线性回归不同，softmax回归的输出单元从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练。

+ 连续值到离散值的转化通常会影响到分类质量，一般使用更加适合离散值输出的模型来解决分类问题。

+ softmax回归的输出值个数等于标签里的类别数，如有4种特征和3种输出动物类别，权重包含12个标量。

+ softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出的计算都要依赖于所有的输入，softmax回归的输出层也是一个全连接层。

+ ![image-20211209182638666](https://s2.loli.net/2021/12/09/WKJEceA4nT6l7tf.png)

+ softmax运算符（softmax operator）解决了直接使用输出层输出的两个问题：

  1) 由于输出层的输出值的范围不确定，难以直观上判断这些值的意义。
  2) 由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。

+ softmax运算不改变预测类别输出<img src="https://s2.loli.net/2021/12/09/bMgyhm3i8qJwUcI.png" alt="image-20211209183050285" style="zoom:33%;" /><img src="https://s2.loli.net/2021/12/09/DQOSVWXozPYiJKF.png" alt="image-20211209183123212" style="zoom: 50%;" />

+ 单样本分类的矢量计算表达式

  <img src="https://s2.loli.net/2021/12/09/cP4azJXBlYpyDf8.png" alt="image-20211209183452752" style="zoom:33%;" />

+ 小批量样本分类的矢量计算表达式

  <img src="C:\Users\Q\AppData\Roaming\Typora\typora-user-images\image-20211209183511510.png" alt="image-20211209183511510" style="zoom:33%;" />

+ 交叉熵（cross entropy）适合衡量两个概率分布的差异。它只关心对正确类别的预测概率，只要其值足够大，就可以确保分类结果正确。

  <img src="C:\Users\Q\AppData\Roaming\Typora\typora-user-images\image-20211209183821244.png" alt="image-20211209183821244" style="zoom:33%;" />

  假设训练数据集的样本数为nn*n*，交叉熵损失函数定义为

  <img src="C:\Users\Q\AppData\Roaming\Typora\typora-user-images\image-20211209184010208.png" alt="image-20211209184010208" style="zoom:33%;" />

  最小化交叉熵损失函数<=>最大化训练数据集所有标签类别的联合预测概率

+ 在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。通常把预测概率最大的类别作为输出类别。如果它与真实类别（标签）一致，说明预测正确。

### 3.5 图像分类数据集（Fashion-MNIST）

+ torchvision包服务于PyTorch深度学习框架，主要用来构建计算机视觉模型。主要由以下几部分构成：
  - `torchvision.datasets`: 一些加载数据的函数及常用的数据集接口；
  - `torchvision.models`: 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；
  - `torchvision.transforms`: 常用的图片变换，例如裁剪、旋转等；
  - `torchvision.utils`: 其他的一些有用的方法
+ 在实践中，数据读取经常是训练的性能瓶颈，特别当模型较简单或者计算硬件性能较高时。PyTorch的`DataLoader`有很方便的功能是允许使用多进程来加速数据读取。

### 3.6 softmax回归的简单从零开始实现

+ 给定一个类别的预测概率分布`y_hat`，把预测概率最大的类别作为输出类别。如果它与真实类别`y`一致，说明这次预测是正确的。分类准确率即正确预测数量与总预测数量之比。
+ 可以使用softmax回归做多类别分类。
+ 绝大多数深度学习模型的训练都有着类似的步骤：
  + 获取并读取数据
  + 定义模型和损失函数
  + 使用优化算法训练模型
+ 在训练模型时，迭代周期数num_epochs和学习率lr都是可以调的超参数。改变它们的值可能会得到分类更准确的模型。

### 3.7 softmax回归的简洁实现

+ 分开定义softmax运算和交叉熵损失函数可能会造成数值不稳定。PyTorch提供了一个包括softmax运算和交叉熵损失计算的函数。其数值稳定性更好。
+ PyTorch提供的函数往往具有更好的数值稳定性

### 3.8 多层感知机MLP

+ multilayer perceptron，MLP
+ 深度学习主要关注多层模型
+ 隐藏层中的神经元和输入层中各个输入完全连接，输出层中的神经元和隐藏层中的各个神经元也完全连接。因此，多层感知机中的隐藏层和输出层都是全连接层。
+ 全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）
  + ReLU（rectified linear unit）函数，ReLU(x) = max(*x*, 0)只保留正数元素，并将负数元素清零。
  + sigmoid函数， sigmoid(*x*) = 1/ 1+ exp(-*x*)可以将元素的值变换到0和1之间，当输入接近0时，sigmoid函数接近线性变换。依据链式法则，sigmoid函数的导数sigmoid'(x) = sigmoid(x) · (1-sigmoid(x))，当输入为0时，sigmoid函数的导数达到最大值0.25；当输入越偏离0时，sigmoid函数的导数越接近0。
  + tanh（双曲正切）函数tanh(x) = (1- exp(2x)) / (1+exp(-2x))，可以将元素的值变换到-1和1之间，当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。tanh函数的导数tanh'(x) = 1- tanh²(x)，当输入为0时，tanh函数的导数达到最大值1；当输入越偏离0时，tanh函数的导数越接近0。
+ 多层感知机是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。`多层感知机的层数`和`各隐藏层中隐藏单元个数`都是超参数。*H = ф (XW~h~ + b~h~)*, *O = HW~o~ + b~o~*
+ 多层感知机在输出层与输入层之间加入了一个或多个全连接隐藏层，并通过激活函数对隐藏层输出进行变换。常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。

### 3.9 多层感知机的从零开始实现

### 3.10 多层感知机的简洁实现

### 3.11 模型选择、欠拟合和过拟合

+ Q：当模型在训练数据集上更准确时，它在测试数据集上却不一定更准确

+ `训练误差（training error）`：模型在训练数据集上表现出的误差

+ `泛化误差（generalization error）`：模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。

+ 一般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现

+ 由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。机器学习模型应关注降低泛化误差。

+ `模型选择（model selection）`评估若干候选模型的表现并从中选择模型，可供选择的候选模型可以是有着不同超参数的同类模型。

+ `验证数据集（validation data set）`：在训练数据集和测试数据集以外的用于进行模型选择的数据，简称`验证集（validation set）`

+ `K折交叉验证(K-fold cross-validation)`

+ `欠拟合（underfitting）`：模型无法得到较低的训练误差

+ `过拟合（overfitting）`：模型的训练误差远小于在测试数据集上的误差

+ 导致这过拟合、欠拟合问题的两个重要因素：

  + 模型复杂度
  + 训练数据集的大小

+ 应对欠拟合和过拟合的办法：

  + 针对数据集选择合适复杂度的模型
  + 避免使用过少的训练样本，特别是在模型复杂度较高时

+ 高阶多项式函数比低阶多项式函数更容易在相同的训练数据集上得到更低的训练误差

  <img src="https://static.sitestack.cn/projects/Dive-into-DL-PyTorch/971d0a255cfd30da05e6ad1bf98b5e6e.svg" alt="img" style="zoom:50%;" />

+ 如果`训练数据集中样本数`过少，特别是比`模型参数数量`（按元素计）更少时，过拟合更容易发生。泛化误差不会随训练数据集里样本数量增加而增大。

+ 线性模型在非线性模型（如三阶多项式函数）生成的数据集上容易欠拟合

+ 使用与数据生成模型同阶的多项式函数模型，如果训练样本不足，该模型依然容易过拟合，训练样本过少甚至少于模型参数的数量，会使模型显得过于复杂，以至于容易被训练数据中的噪声影响，在迭代过程中，尽管训练误差较低，但是测试数据集上的误差却很高，是典型的过拟合现象。

### 3.12 权重衰减

+ 应对过拟合问题的常用方法：权重衰减（weight decay）
+ 权重衰减**等价于**L~2~范数正则化（regularization），通常会使学到的权重参数的元素较接近0。
+ 正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。
+ L~2~范数正则化在模型原损失函数基础上添加L~2~范数惩罚项，从而得到训练所需要最小化的函数
+ 权重衰减可以通过优化器中的`weight_decay`超参数来指定。
+ 可以定义多个优化器实例对不同的模型参数使用不同的迭代方法。

### 3.13 丢弃法

+ 深度学习模型常常使用丢弃法（dropout）来应对过拟合问题
+ 倒置丢弃法（inverted dropout）
+ 丢弃概率*p*是丢弃法的==超参数==，丢弃法不改变其输入的期望值。
+ 在测试模型时，为了拿到更加确定性的结果，一般不使用丢弃法。

### 3.14 正向传播、反向传播和计算图

+ 正向传播是指对神经网络沿着从输入层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）

+ 反向传播指的是计算神经网络参数梯度的方法。依据微积分中的链式法则，沿着从输出层到输入层的顺序，依次计算并存储目标函数有关神经网络各层的中间变量以及参数的梯度。

+ 在训练深度学习模型时，正向传播和反向传播之间相互依赖。

  + 正向传播的计算可能依赖于模型参数的当前值，而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的。
  + 反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的。

+ 在模型参数初始化完成后交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。

  既然在反向传播中使用了正向传播中计算得到的中间变量来避免重复计算，那么这个复用也导致正向传播结束后不能立即释放中间变量内存。这也是训练要比预测占用更多内存的一个重要原因。

+ 中间变量的个数大体上与网络层数线性相关，每个变量的大小跟批量大小和输入个数也是线性相关的，它们是导致较深的神经网络使用较大批量训练时更容易超内存的主要原因

### 3.15 数值稳定性和模型初始化

+ 深度模型有关数值稳定性的典型问题是`衰减（vanishing）`和`爆炸（explosion）`
+ 神经网络的层数较多时，模型的数值稳定性容易变差。假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入X分别与0.2^30^≈1×10^−21^（衰减）和5^30^≈9×10^20^（爆炸）的乘积。类似地，当层数较多时，梯度的计算也更容易出现衰减或爆炸。
+ 通常需要随机初始化神经网络的模型参数，如权重参数。

## 4. 深度学习计算

### 4.1 模型构造

+ `Sequential`类可以接收一个子模块的有序字典（OrderedDict）或者一系列子模块作为参数来逐一添加`Module`的实例，模型的前向计算就是将这些实例按添加的顺序逐一计算。

+ `ModuleList`接收一个子模块的列表作为输入，然后也可以类似List那样进行append和extend操作。

+ `ModuleDict`实例仅仅是存放了一些模块的字典，并没有定义`forward`函数

+ 与`Sequential`不同，`ModuleList`和`ModuleDict`并没有定义一个完整的网络，它们只是将不同的模块存放在一起，需要自己定义`forward`函数。

  `Sequential`和`ModuleList`都可以进行列表化构造网络。`ModuleList`仅是一个储存各种模块的列表，模块之间没有联系也没有顺序（所以不用保证相邻层的输入输出维度匹配）；`Sequential`内的模块需要按照顺序排列，要保证相邻层的输入输出大小相匹配。`ModuleList`的出现只是让网络定义前向传播时更加灵活。

### 4.2 模型参数的访问、初始化共享

+ 访问和初始化模型参数，在多个层之间共享同一份模型参数
+ 共享模型参数: 
  + `Module`类的`forward`函数里多次调用同一个层
  + 传入`Sequential`的模块是同一个`Module`实例
+ 因为模型参数里包含了梯度，所以在反向传播计算时，这些共享的参数的梯度是累加的

### 4.3 模型参数的延后初始化

### 4.4 自定义层

+ 可以通过`Module`类自定义神经网络中的层，从而可以被重复调用。

### 4.5 读取和存储

+ 直接使用`save`函数和`load`函数分别存储和读取`Tensor`
+ `save`使用Python的pickle实用程序将对象进行序列化，然后将序列化的对象保存到disk，使用`save`可以保存各种对象,包括模型、张量和字典等
+ `load`使用pickle unpickle工具将pickle的对象文件反序列化为内存

## 5. 卷积神经网络

### 5.1 二维卷积层

+ `卷积神经网络（convolutional neural network）`
+ `卷积层（convolutional layer）`
+ 二维卷积层有高和宽两个空间维度，常用来处理图像数据。
+ 在二维卷积层中，一个二维输入数组和一个`二维核（kernel）`数组通过`互相关运算`输出一个二维数组。核数组在卷积计算中又称`卷积核`或过滤器（filter）。
+ 二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。
+ `卷积层的模型参数`包括了`卷积核`和`标量偏差`。在训练模型的时候，通常先对卷积核随机初始化，然后不断迭代卷积核和偏差。
+ 卷积层可通过重复使用卷积核有效地表征局部空间
+ 卷积运算与互相关运算类似。**为了得到卷积运算的输出，只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算**。
+ 二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫`特征图（feature map）`。影响元素x的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做x的`感受野（receptive field）`。
+ 可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。

### 5.2 填充和步幅

+ 输入形状n~h~×n~w~，卷积核形状k~h~×k~w~，输出形状(n~h~ - k~h~ + 1 )×(n~w~ - k~w~ + 1)所以卷积层的输出形状由输入形状和卷积核窗口形状决定。

+ `填充`和`步幅`，可以对给定形状的输入和卷积核改变输出形状。

+ `填充（padding）`是指==在输入的高和宽的两侧==填充元素（通常是0）。如果在高的两侧一共填充p~h~行，在宽的两侧一共填充p~w~列，则输出形状将会是==(n~h~−k~h~+p~h~+1)×(n~w~−k~w~+p~w~+1)==，即输出的高和宽会分别增加p~h~和p~w~<img src="https://s2.loli.net/2021/12/21/23PykmQIeot4xbJ.png" alt="image-20211221102043238" style="zoom: 50%;" />

+ 在很多情况下会设置p~h~=k~h~-1和p~w~=k~w~-1，则输出形状将会是==n~h~×n~w~==，使输入和输出具有相同的高和宽。这样会方便在构造网络时推测每个层的输出形状。

  若k~h~是奇数，在高的两侧分别填充p~h~/2行。

  若k~h~是偶数，一种可能是在输入的顶端一侧填充⌈p~h~/2⌉行，而在底端一侧填充⌊p~h~/2⌋行。在宽的两侧填充同理。

+ 卷积神经网络经常使用奇数高宽的卷积核，所以两端上的填充个数相等。

+ 当两端上的填充个数相等，并使输入和输出具有相同的高和宽时，即知输出`Y[i,j]`是由输入以`X[i,j]`为中心的窗口与卷积核进行互相关计算得到的。

+ 卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动并进行二位互相关运算。将每次滑动的行数和列数称为`步幅（stride）`，图为在高上步幅为3，在宽上步幅为2的二维互相关运算。

  <img src="C:\Users\Q\AppData\Roaming\Typora\typora-user-images\image-20211221103730942.png" alt="image-20211221103730942" style="zoom:50%;" />

+ 当高上步幅为s~h~，宽上步幅为s~w~时，输出形状为==⌊(n~h~−k~h~+p~h~+s~h~)/s~h~⌋×⌊(n~w~−k~w~+p~w~+s~w~)/s~w~⌋==。

+ 若p~h~=k~h~-1和p~w~=k~w~-1，则输出形状简化为⌊(n~h~+s~h~−1)/s~h~⌋×⌊(n~w~+s~w~−1)/s~w~⌋。更进一步，若n~h~、n~w~能分别被s~h~、s~w~整除，则输出形状是(n~h~/s~h~)×(n~w~/s~w~)

+ 填充(p~h~, p~w~)；步幅(s~h~, s~w~)。默认情况下，填充为0，步幅为1。

### 5.3 多输入通道和多输出通道

+ `通道（channel）`维

+ 使用多通道可以拓展卷积层的模型参数。

+ 当输入数据含多（n）个通道时，需要构造具备n个输入通道数的卷积核，从而能与含n个通道的输入数据做互相关运算。在各个通道上对输入的二维数组和卷积核的二维核数组做互相关运算，再将这n个互相关运算的二维输出按通道相加，得到一个二维数组作为输出。

  ![image-20211221110051617](https://s2.loli.net/2021/12/21/YXHDNZpuef7nPkL.png)

+ 设卷积核输入通道数和输出通道数分别为c~i~和c~o~，高和宽分别为k~h~和k~w~。如果希望得到含多个通道的输出，可以为每个输出通道分别创建形状为c~i~×k~h~×k~w~的核数组。将它们在输出通道维上连结，卷积核的形状即c~o~× c~i~× k~h~× k~w~。在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与**整个输入数组**计算而来。

+ 卷积窗口形状为1×1（k~h~=k~w~=1）的多通道卷积层，通常称为`1×1卷积层`，并将其中的卷积运算称为`1×1卷积`。因为使用了最小窗口，1×1卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上1×1卷积的主要计算发生在通道维上。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加

+ 假设将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么1×1卷积层的作用与全连接层等价。

+ 1×1卷积层通常用来调整网络层之间的通道数，并控制模型复杂度。

### 5.4 池化层

+ `池化（pooling）层`的一个主要作用是缓解卷积层对位置的过度敏感性。
+ 池化层每次对输入数据的一个固定形状窗口（`池化窗口`，p×q池化层）中的元素计算输出。池化层直接计算池化窗口内元素的最大值或者平均值，即`最大池化`或`平均池化`，池化运算叫作`p×q池化`
+ 池化层也可以指定填充和步幅来改变输出形状，工作机制与卷积层相同。
+ **池化层对每个输入通道分别池化**，即池化层输出通道数等于输入通道数

### 5.5 卷积神经网络（LeNet）

+ 卷积神经网络就是含卷积层的网络。

+ LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。

  在卷积层块中输入的高和宽在逐层减小。卷积层由于使用高和宽均为5的卷积核，从而将高和宽分别减小4。

  而池化层则将高和宽减半，但通道数则从1增加到16。

  全连接层则逐层减少输出个数，直到变成图像的类别数10。

  ```python
  LeNet(
    # convolutions 全连接层
    (conv): Sequential(
      (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
      (1): Sigmoid()
      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
      (4): Sigmoid()
      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    # full connection 全连接层
    (fc): Sequential(
      (0): Linear(in_features=256, out_features=120, bias=True)
      (1): Sigmoid()
      (2): Linear(in_features=120, out_features=84, bias=True)
      (3): Sigmoid()
      (4): Linear(in_features=84, out_features=10, bias=True)
    )
  )
  ```

### 5.6 深度卷积神经网络（AlexNet）

+ 神经网络可以直接基于图像的原始像素进行分类。这种称为端到端（end-to-end）的方法节省了很多中间步骤。
+ [AlexNet](Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105).)使用了8层卷积神经网络![image-20211222095533813](https://s2.loli.net/2021/12/22/SJqvPnD6N7Emp8l.png)

+ ```python
  AlexNet(
    (conv): Sequential(
      (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))
      (1): ReLU()
      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
      (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
      (4): ReLU()
      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
      (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): ReLU()
      (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (9): ReLU()
      (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU()
      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (fc): Sequential(
      (0): Linear(in_features=6400, out_features=4096, bias=True)
      (1): ReLU()
      (2): Dropout(p=0.5, inplace=False)
      (3): Linear(in_features=4096, out_features=4096, bias=True)
      (4): ReLU()
      (5): Dropout(p=0.5, inplace=False)
      (6): Linear(in_features=4096, out_features=10, bias=True)
    )
  )
  ```

+ AlexNet第一层中的卷积窗口形状是 11×11。用更大的卷积窗口来捕获物体。第二层中的卷积窗口形状减小到 5×5，之后全采用 3×3。

  第1、2、5个卷积层之后都 用了窗口形状为 3×3、步幅为2的最大池化层。且AlexNet使用的卷积通道数也大于LeNet中的卷积通道数数十倍。

  AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。

  1. ReLU激活函数的计算更简单。
  2. ReLU激活函数在不同的参数初始化方法下使模型更容易训练。ReLU在正区间的梯度恒为1；sigmoid输出极接近0或1时，这些区域的梯度几乎为0，导致反向传播无法继续更新部分模型参数，若模型参数初始化不当，sigmoid可能会在正区间得到几乎为0的梯度，导致模型无法有效训练。

  AlexNet通过丢弃法（ch 3.13）来控制全连接层的模型复杂度。

### 5.7 使用重复元素的网络（VGG）

+ VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路
+ VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为3×3的卷积层后接上一个步幅为2、窗口形状为2×2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。
+ 对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于采用大的卷积核，因为可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。例如，在VGG中，使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5*5卷积核，这样做的主要目的是在保证具有相同感知野的条件下，提升了网络的深度，在一定程度上提升了神经网络的效果。
+ 每次将输入的高和宽减半，直到最终高和宽变成7后传入全连接层。与此同时，输出通道数每次翻倍，直到变成512。因为每个卷积层的窗口大小一样，所以每层的模型参数尺寸和计算复杂度与输入高、输入宽、输入通道数和输出通道数的乘积成正比。
+ 

### 5.8 网络中的网络（NiN）

+ 串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。
+ 卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维。
+ 1×1卷积层可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，NiN使用1×1卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。<img src="https://s2.loli.net/2021/12/24/LQjYMSGJlThPp6g.png" alt="image-20211224215314930" style="zoom:80%;" />
+ NiN使用卷积窗口形状分别为 11×11、 5×5和 3×3的卷积层，相应的输出通道数也与AlexNet中的一致。每个NiN块后接一个步幅为2、窗口形状为 3×3的最大池化层。
+ NiN去除了容易造成过拟合的全连接输出层，使用输出通道数等于标签类别数的NiN块和`全局平均池化层(窗口形状等于输入空间维形状的平均池化层)`。
  + 优点：可以显著减小模型参数尺寸，从而缓解过拟合
  + 缺点：有时会造成获得有效模型的训练时间的增加

### 5.9 含并行连结的网络（GoogLeNet）

+ GoogLeNet中的基础卷积块叫作Inception块。相当于一个有4条线路的子网络。它通过不同窗口形状的卷积层和最大池化层来并行抽取信息，并使用 1×1 卷积层减少通道数从而降低模型复杂度。
+ Inception块里有4条并行的线路。前3条线路使用窗口大小分别是 1×1、 3×3和 5×5的卷积层来抽取不同空间尺寸下的信息，其中线路2、3会对输入先做 1×1卷积来减少输入通道数，以降低模型复杂度。线路4则使用 3×3最大池化层，后接 1×1 卷积层来改变通道数。四条线路都使用了合适的填充来使输入与输出的高和宽一致。最后将每条线路的输出在通道维上连结，并输入接下来的层中。![image-20211224220612309](https://s2.loli.net/2021/12/24/CDdvYNLmX5aKo7I.png)
+ Inception块中可以自定义的超参数是每个层的输出通道数，以此控制模型复杂度。

### 5.10 批量归一化

+ `批量归一化（batch normalization）`层能让较深的神经网络的训练变得更加容易。
+ 标准化处理输入数据使各个特征的分布相近往往更容易训练出有效模型。
+ 数据标准化预处理
  + 浅层模型：足够有效。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。
  + 深层神经网络：即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常导致难以训练出有效的深度模型。
+ ==批量归一化的提出是为了应对深度模型训练的挑战==。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。![image-20211227100938284](https://s2.loli.net/2021/12/27/WNMhDmFras3CVzG.png)
+ 对全连接层和卷积层做批量归一化的方法稍有不同：
  + 全连接层批量：将批量归一化层置于全连接层中的仿射变换和激活函数之间。使用批量归一化的全连接层的输出为*ϕ*(BN(**x**))。若批量归一化无益，理论上，学出的模型可以不使用批量归一化。
  + 卷积层：批量归一化发生在卷积计算之后、应用激活函数之前。若卷积计算输出多个通道，则需要对这些通道的输出分别做批量归一化，且**每个通道都有独立的拉伸和偏移参数且均为标量**。
+ 批量归一化层在训练模式和预测模式下的计算结果不一样。
+ PyTorch提供了BatchNorm类方便使用。

### 5.11 残差网络（ResNet）

+ 问题：由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。但实践添加过多的层后训练误差往往不降反升。即使利用批量归一化带来的数值稳定性使训练深层模型更容易，该问题依然存在。
+ [残差网络ResNet](He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).)
+ 假设希望学出的理想映射为*f*(**x**)，从而作为图5.9上方激活函数的输入。左图虚线框中的部分需要直接拟合出该映射*f*(**x**)，而右图虚线框中的部分则需要拟合出有关恒等映射的残差映射*f*(**x**)−**x**。残差映射在实际中往往更容易优化。在残差块`（residual block）`中，输入可通过跨层的数据线路更快地向前传播。<img src="https://s2.loli.net/2021/12/27/AIXp34nLyfd8Uvl.png" alt="image-20211227101949200" style="zoom: 33%;" />
+ ResNet沿用了VGG全 3×3 卷积层的设计。残差块里首先有2个有相同输出通道数的 3×3 卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。然后把**原始输入**跳过卷积运算直接加在最后的ReLU激活函数前。此设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的 1×1 卷积层将输入变换成需要的形状后再做相加运算。
+ 残差块可以设定输出通道数、是否使用额外的 1×1 卷积层来修改通道数以及卷积层的步幅。
+ 残差块通过跨层的数据通道从而能够训练出有效的深度神经网络。

### 5.12 稠密连接网络（DenseNet）

+ DenseNet的主要构建模块是`稠密块(dense block)`和`过渡层(transition layer)`。前者定义了输入和输出是如何连结的，后者则用来控制通道数，使之不过大。

  <img src="https://s2.loli.net/2021/12/27/m6nUS7lbGuCVBOW.png" alt="image-20211227103953645" style="zoom: 33%;" />

+ 卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此被称为`增长率(growth rate)`

